{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio in glob('./wav/*/*/*.wav'):\n",
    "    new_audio = audio.split('\\\\')[-1]\n",
    "    shutil.move(audio, f'./audio/{new_audio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=['session', 'type_num', 'gender_num'])\n",
    "valid_df = pd.DataFrame(columns=['session', 'type_num', 'gender_num'])\n",
    "test_df = pd.DataFrame(columns=['session', 'type_num', 'gender_num'])\n",
    "\n",
    "for audio in os.listdir('./audio'):\n",
    "    session, type, gender_num  = audio.split('_')\n",
    "    gender_num = gender_num.split('.')[0]\n",
    "    if session in ['Sess19','Sess20']:\n",
    "        test_df = train_df.append({'session':session, 'type_num':type, 'gender_num':gender_num}, ignore_index=True)\n",
    "    elif session in ['Sess17', 'Sess18']:\n",
    "        valid_df = valid_df.append({'session':session, 'type_num':type, 'gender_num':gender_num}, ignore_index=True)\n",
    "    else:\n",
    "        train_df = train_df.append({'session':session, 'type_num':type, 'gender_num':gender_num}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv', index=False)\n",
    "valid_df.to_csv('valid.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기부터 연습코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49962,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = librosa.load('./audio/Sess01_impro01_F001.wav', sr=16000)\n",
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58800,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = librosa.load('./audio/Sess01_impro01_F003.wav', sr=16000)\n",
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.006073  ,  0.00592041,  0.0017395 , ..., -0.02334595,\n",
       "       -0.02182007, -0.01806641], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0007, -0.0010,  0.0006,  ..., -0.0060, -0.0040,  0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(audio).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0061,  0.0059,  0.0017,  ..., -0.0233, -0.0218, -0.0181])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import EmotionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = EmotionDataset(split='valid', num=1)\n",
    "loader = DataLoader(data, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': array([-0.00189209, -0.0010376 , -0.00164795, ...,  0.        ,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 'speaker': 6,\n",
       " 'listener': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[-0.0019, -0.0010, -0.0016,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0007, -0.0008, -0.0006,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0013, -0.0009, -0.0007,  ...,  0.0000,  0.0000,  0.0000]]), 'speaker': tensor([6, 4, 4]), 'listener': tensor([5, 5, 2])}\n"
     ]
    }
   ],
   "source": [
    "for a in loader:\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin = r.randint(10)\n",
    "begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = pd.read_csv('./C.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['surprise'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot[(annot['Segment ID']=='Sess01_script01_M001') & (annot['role']=='speaker')]['Emotion.1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disgust': 0,\n",
       " 'angry': 1,\n",
       " 'sad': 2,\n",
       " 'fear': 3,\n",
       " 'surprise': 4,\n",
       " 'neutral': 5,\n",
       " 'happy': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {'disgust':0, 'angry':1, 'sad':2, 'fear':3, \n",
    "'surprise':4,'neutral':5, 'happy':6}\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_batch_first=True)\n",
    "src = torch.rand(3, 512)\n",
    "out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32md:\\KEMDy19\\EDA.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./textdata_segID.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000027?line=1'>2</a>\u001b[0m     f\u001b[39m.\u001b[39;49mreadlines()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch/lib/codecs.py?line=318'>319</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch/lib/codecs.py?line=319'>320</a>\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch/lib/codecs.py?line=320'>321</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/anaconda3/envs/torch/lib/codecs.py?line=321'>322</a>\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch/lib/codecs.py?line=322'>323</a>\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch/lib/codecs.py?line=323'>324</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open('./textdata_segID.txt', 'r', encoding='cpc') as f:\n",
    "    f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./textdata_segID.txt', 'rb') as lf:\n",
    "    text_ID = pickle.load(lf)\n",
    "\n",
    "with open('./embedding_textdata.txt', 'rb') as lf:\n",
    "    embedding = pickle.load(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10284"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10284"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb_dic = {}\n",
    "for id, emb in zip(text_ID, embedding):\n",
    "    text_emb_dic[id] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('text_emb_dict.npy', text_emb_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sess17_impro01_F001'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\KEMDy19\\EDA.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000033?line=0'>1</a>\u001b[0m text_emb_dic \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mtext_emb_dic.npy\u001b[39m\u001b[39m'\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000033?line=1'>2</a>\u001b[0m text_emb_dic[\u001b[39m'\u001b[39;49m\u001b[39mSess17_impro01_F001\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sess17_impro01_F001'"
     ]
    }
   ],
   "source": [
    "text_emb_dic = np.load('text_emb_dic.npy', allow_pickle=True).item()\n",
    "text_emb_dic['Sess17_impro01_F001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8265"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_emb_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import EmotionDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EmotionDataset('test', 1)\n",
    "train_loader = DataLoader(train_data, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pack in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pack['audio']\n",
    "text = pack['text']\n",
    "emo_s = pack['speaker']\n",
    "emo_l = pack['listener']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0012,  0.0010, -0.0007,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0007, -0.0006,  0.0016,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0061,  0.0059,  0.0017,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[   2, 3206, 6844, 3945, 5595, 6150, 1258, 5793, 6060, 6393,  517,   54,\n",
      "         3220, 7389, 7767, 2584, 7123, 3105, 7328, 2049, 5761, 1185, 6043, 1189,\n",
      "          517, 5330, 5859, 1765,  862, 3098,  633,    3,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2, 1198, 5771,  517, 7226, 5330, 6015, 2298, 5357, 6016, 5400,  517,\n",
      "           46, 3220, 3502, 1934, 7318, 2298, 7126, 7100,  517, 6005, 1407, 7101,\n",
      "         6553, 4249, 7871, 5377, 6844,  633,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   2, 3135, 5886, 5406, 3429, 6022, 3742, 2086, 1933, 5406,  517,   54,\n",
      "            3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]], dtype=torch.int32)\n",
      "tensor([1, 5, 1])\n",
      "tensor([5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(audio)\n",
    "print(text)\n",
    "print(emo_s)\n",
    "print(emo_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 160000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.6159e-01, -7.6159e-01, -0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "          -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6159e-01,\n",
       "           0.0000e+00, -7.6159e-01,  7.6159e-01, -7.6159e-01, -7.6159e-01,\n",
       "          -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           7.5194e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00, -7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "           7.6159e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           7.6159e-01,  0.0000e+00, -7.6159e-01,  7.6159e-01,  0.0000e+00,\n",
       "           7.6159e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00,  7.6159e-01, -0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -0.0000e+00, -7.6159e-01,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "          -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6159e-01,\n",
       "           7.6159e-01,  0.0000e+00,  0.0000e+00],\n",
       "         [ 9.6403e-01, -7.6159e-01,  0.0000e+00, -0.0000e+00, -7.6159e-01,\n",
       "          -0.0000e+00,  0.0000e+00,  2.3196e-19,  0.0000e+00, -9.6403e-01,\n",
       "           0.0000e+00, -9.6403e-01,  7.6159e-01,  0.0000e+00, -9.6403e-01,\n",
       "          -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           7.6159e-01, -7.6159e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           7.6159e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.6403e-01,\n",
       "           1.7717e-02,  7.6159e-01,  9.6403e-01, -7.6159e-01, -9.6403e-01,\n",
       "           0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01, -7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00, -7.6159e-01,\n",
       "           9.6403e-01,  0.0000e+00,  1.2814e-01,  0.0000e+00, -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -9.6403e-01,  0.0000e+00,  0.0000e+00,\n",
       "           9.6360e-01, -6.4594e-05, -7.6159e-01, -7.6159e-01,  0.0000e+00,\n",
       "           7.6159e-01, -0.0000e+00, -7.6159e-01, -0.0000e+00,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           7.6159e-01, -9.6403e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "          -7.6159e-01,  0.0000e+00,  7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -0.0000e+00, -7.6159e-01,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -0.0000e+00, -7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "          -9.6403e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.6403e-01,\n",
       "           7.6159e-01,  0.0000e+00,  0.0000e+00],\n",
       "         [ 9.6403e-01, -7.8148e-01,  0.0000e+00, -0.0000e+00, -7.6159e-01,\n",
       "          -9.6403e-01, -0.0000e+00,  8.4635e-01,  0.0000e+00, -0.0000e+00,\n",
       "          -7.6159e-01, -0.0000e+00,  7.6159e-01,  7.6159e-01,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  9.6403e-01,\n",
       "           0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           7.6159e-01, -0.0000e+00, -0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "           7.5960e-01,  0.0000e+00,  1.0116e-06,  1.0149e-23, -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5378e-38, -9.9505e-01,\n",
       "           7.6894e-01,  9.6403e-01,  7.6159e-01, -0.0000e+00, -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "          -0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,\n",
       "           9.6403e-01,  0.0000e+00,  0.0000e+00,  7.6159e-01, -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00, -9.9505e-01,  7.6159e-01,  0.0000e+00,\n",
       "           7.5905e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -0.0000e+00, -0.0000e+00,  0.0000e+00, -7.6159e-01,  9.6403e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "           7.6159e-01, -9.9505e-01,  0.0000e+00, -9.3934e-13,  7.6159e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01, -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -9.6403e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -0.0000e+00, -7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "          -0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01, -0.0000e+00,\n",
       "           7.1011e-01,  2.5513e-01,  0.0000e+00]], grad_fn=<SqueezeBackward1>),\n",
       " (tensor([[ 9.6403e-01, -7.8148e-01,  0.0000e+00, -0.0000e+00, -7.6159e-01,\n",
       "           -9.6403e-01, -0.0000e+00,  8.4635e-01,  0.0000e+00, -0.0000e+00,\n",
       "           -7.6159e-01, -0.0000e+00,  7.6159e-01,  7.6159e-01,  7.6159e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  9.6403e-01,\n",
       "            0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "            7.6159e-01, -0.0000e+00, -0.0000e+00,  0.0000e+00,  7.6159e-01,\n",
       "            7.5960e-01,  0.0000e+00,  1.0116e-06,  1.0149e-23, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5378e-38, -9.9505e-01,\n",
       "            7.6894e-01,  9.6403e-01,  7.6159e-01, -0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,\n",
       "            9.6403e-01,  0.0000e+00,  0.0000e+00,  7.6159e-01, -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00, -9.9505e-01,  7.6159e-01,  0.0000e+00,\n",
       "            7.5905e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00,  0.0000e+00, -7.6159e-01,  9.6403e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00,  0.0000e+00,\n",
       "            7.6159e-01, -9.9505e-01,  0.0000e+00, -9.3934e-13,  7.6159e-01,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01, -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -9.6403e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -0.0000e+00, -7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01, -0.0000e+00,\n",
       "            7.1011e-01,  2.5513e-01,  0.0000e+00]], grad_fn=<SqueezeBackward1>),\n",
       "  tensor([[ 2.0000e+00, -1.0492e+00,  1.0000e+00, -0.0000e+00, -1.0000e+00,\n",
       "           -2.0000e+00, -1.0000e+00,  1.2431e+00,  0.0000e+00, -1.0000e+00,\n",
       "           -1.0000e+00, -2.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3028e-05,  2.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  1.0000e+00, -0.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0116e-06,  1.4283e-20, -1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00, -3.0000e+00,\n",
       "            1.0177e+00,  2.0000e+00,  1.0000e+00, -0.0000e+00, -3.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  7.0854e-08,  1.0000e+00, -2.0000e+00,\n",
       "           -1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
       "            2.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00, -0.0000e+00,\n",
       "            0.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -1.0000e+00, -3.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            9.9398e-01, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -1.7614e-25, -0.0000e+00,  0.0000e+00, -1.0000e+00,  2.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00, -3.0000e+00,  0.0000e+00, -1.0000e+00,  1.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00, -1.0000e+00,\n",
       "            0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            9.3042e-01, -2.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
       "           -1.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00, -1.0000e+00,\n",
       "            1.0000e+00,  2.6090e-01,  0.0000e+00]], grad_fn=<SqueezeBackward1>)))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "lstm = nn.LSTM(64, 128, 1, batch_first=True)\n",
    "lstm(text.type(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "\n",
    "spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_SIZE,\n",
    "            f_min=F_MIN,\n",
    "            f_max=F_MAX,\n",
    "            n_mels=N_MELS,\n",
    "            normalized=False)\n",
    "\n",
    "to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "spec_bn = nn.BatchNorm2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 229, 313])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb = spec(audio)\n",
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 229, 313])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb = to_db(audio_emb)\n",
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 229 elements not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\KEMDy19\\EDA.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000048?line=0'>1</a>\u001b[0m audio_emb \u001b[39m=\u001b[39m spec_bn(audio_emb)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000048?line=1'>2</a>\u001b[0m audio_emb\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=160'>161</a>\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=162'>163</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=163'>164</a>\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=164'>165</a>\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=165'>166</a>\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=166'>167</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=167'>168</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=168'>169</a>\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=169'>170</a>\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=170'>171</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=171'>172</a>\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=172'>173</a>\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=173'>174</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=174'>175</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=175'>176</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=176'>177</a>\u001b[0m     bn_training,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=177'>178</a>\u001b[0m     exponential_average_factor,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=178'>179</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/batchnorm.py?line=179'>180</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/functional.py?line=2417'>2418</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/functional.py?line=2418'>2419</a>\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/functional.py?line=2420'>2421</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/functional.py?line=2421'>2422</a>\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/functional.py?line=2422'>2423</a>\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 229 elements not 1"
     ]
    }
   ],
   "source": [
    "audio_emb = spec_bn(audio_emb)\n",
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv1d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0):\n",
    "        super(BasicConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        return x\n",
    "\n",
    "audio_emb_layer = nn.Sequential(\n",
    "            BasicConv1d(1, 64, (1, 3)),\n",
    "            BasicConv1d(64, 100, (1, 5)),\n",
    "            BasicConv1d(100, 100, (1, 7))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 1, 1, 3], but got 3-dimensional input of size [1, 3, 160000] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\KEMDy19\\EDA.ipynb Cell 41'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000050?line=0'>1</a>\u001b[0m audio_emb \u001b[39m=\u001b[39m audio_emb_layer(audio)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000050?line=1'>2</a>\u001b[0m audio_emb\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\KEMDy19\\EDA.ipynb Cell 40'\u001b[0m in \u001b[0;36mBasicConv1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000049?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000049?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000049?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KEMDy19/EDA.ipynb#ch0000049?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=300'>301</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=301'>302</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch2\\lib\\site-packages\\torch\\nn\\modules\\conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=293'>294</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=294'>295</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=295'>296</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=296'>297</a>\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=297'>298</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/user/anaconda3/envs/torch2/lib/site-packages/torch/nn/modules/conv.py?line=298'>299</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 1, 1, 3], but got 3-dimensional input of size [1, 3, 160000] instead"
     ]
    }
   ],
   "source": [
    "audio_emb = audio_emb_layer(audio)\n",
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 160000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "annot = pd.read_csv('./C.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = annot[(annot['Segment ID']=='Sess01_script01_M001') & (annot['role']=='speaker')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_list = []\n",
    "for i in range(10):\n",
    "    emo = data['Emotion.'+ str(i+1)].values[0]\n",
    "    emo_list.append(label_dict[emo])\n",
    "    \n",
    "\n",
    "    # print(data['Emotion.'+ str(i+1)].values)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 4, 3, 4, 4, 4, 4, 4]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'disgust':0, 'angry':1, 'sad':2, 'fear':3, 'surprise':4,'neutral':5, 'happy':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_label = np.zeros(7)\n",
    "for i in range(7):\n",
    "    soft_label[i] = emo_list.count(i)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0.1, 0.9, 0. , 0. ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 여기부터 성래가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Speaker, Listener 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10279\n",
      "10287\n",
      "20566 20566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/KEMDy19/annotation.csv')\n",
    "del df['Unnamed: 0'], df['Unnamed: 0.1'] , df['Unnamed: 0.1.1']\n",
    "\n",
    "df_speaker = df[df['role']=='speaker']\n",
    "df_listener = df[df['role']=='listener']\n",
    "\n",
    "print(len(df_speaker)) # 10279\n",
    "print(len(df_listener)) # 10287\n",
    "print(len(df),len(df_listener)+len(df_speaker)) # 같이야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Emotion.1</th>\n",
       "      <th>Valence.1</th>\n",
       "      <th>Arousal.1</th>\n",
       "      <th>Emotion.2</th>\n",
       "      <th>...</th>\n",
       "      <th>Emotion.10</th>\n",
       "      <th>Valence.10</th>\n",
       "      <th>Arousal.10</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>role</th>\n",
       "      <th>Session</th>\n",
       "      <th>script</th>\n",
       "      <th>sequence</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start, end, Segment ID, Emotion, Valence, Arousal, Emotion.1, Valence.1, Arousal.1, Emotion.2, Valence.2, Arousal.2, Emotion.3, Valence.3, Arousal.3, Emotion.4, Valence.4, Arousal.4, Emotion.5, Valence.5, Arousal.5, Emotion.6, Valence.6, Arousal.6, Emotion.7, Valence.7, Arousal.7, Emotion.8, Valence.8, Arousal.8, Emotion.9, Valence.9, Arousal.9, Emotion.10, Valence.10, Arousal.10, speaker, text, role, Session, script, sequence, dialogue]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 43 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listener = df_listener.drop(df_listener[df_listener['Segment ID']=='Sess04_impro03_F031'].index)\n",
    "df_listener = df_listener.drop(df_listener[df_listener['Segment ID']=='Sess04_impro03_M031'].index)\n",
    "df_listener[df_listener['Segment ID']=='Sess04_impro03_F031']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speaker.to_csv('./data/KEMDy19/df_speaker.csv',index=False,encoding='utf-8')\n",
    "df_listener.to_csv('./data/KEMDy19/df_listener.csv',index=False,encoding='utf-8')\n",
    "\n",
    "# df_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Emotion.1</th>\n",
       "      <th>Valence.1</th>\n",
       "      <th>Arousal.1</th>\n",
       "      <th>Emotion.2</th>\n",
       "      <th>...</th>\n",
       "      <th>Emotion.10</th>\n",
       "      <th>Valence.10</th>\n",
       "      <th>Arousal.10</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>role</th>\n",
       "      <th>Session</th>\n",
       "      <th>script</th>\n",
       "      <th>sequence</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.943</td>\n",
       "      <td>32.167</td>\n",
       "      <td>Sess01_script01_F002</td>\n",
       "      <td>fear</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>fear</td>\n",
       "      <td>...</td>\n",
       "      <td>fear</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>지섭씨. 일단 112에 신고하자.</td>\n",
       "      <td>speaker</td>\n",
       "      <td>Sess01</td>\n",
       "      <td>script01</td>\n",
       "      <td>F002</td>\n",
       "      <td>Sess01_script01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    start     end            Segment ID Emotion  Valence  Arousal Emotion.1  \\\n",
       "4  28.943  32.167  Sess01_script01_F002    fear      1.6      3.7      fear   \n",
       "\n",
       "   Valence.1  Arousal.1 Emotion.2  ...  Emotion.10  Valence.10 Arousal.10  \\\n",
       "4          3          4      fear  ...        fear           2          3   \n",
       "\n",
       "   speaker                text     role  Session    script sequence  \\\n",
       "4        F  지섭씨. 일단 112에 신고하자.  speaker   Sess01  script01     F002   \n",
       "\n",
       "          dialogue  \n",
       "4  Sess01_script01  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_speaker = pd.read_csv('./data/KEMDy19/df_speaker.csv')\n",
    "\n",
    "df_speaker[df_speaker['Segment ID']=='Sess01_script01_F002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = torch.zeros((7))\n",
    "emotion[2] += 1\n",
    "emotion[2] += 1\n",
    "emotion = emotion/10\n",
    "emotion = torch.FloatTensor(emotion)\n",
    "type(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp = pd.DataFrame(columns=['seg'])\n",
    "temp = temp.append({'seg' : ['1','2']},ignore_index=True)\n",
    "temp['seg'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ws=8일때 데이터 리스트 뽑아 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ws_list(df,ws):\n",
    "        ws_df = pd.DataFrame(columns=['seq'])\n",
    "        ws_list = ['padding' for _ in range(0,ws-1)]\n",
    "        seed_id = df['Segment ID'][1]\n",
    "        for id in df['Segment ID']:\n",
    "            if seed_id.split('_')[0:2] == id.split('_')[0:2]: #같은 dialog라면\n",
    "                ws_list.append(id)\n",
    "                seed_id = id\n",
    "            else:\n",
    "                # ws 만큼 잘라서 ws_df에 append\n",
    "                for i in range(0,len(ws_list)-ws+1):\n",
    "                    ws_df = ws_df.append({'seq' : ws_list[i:i+ws]},ignore_index=True)\n",
    "                ws_list = ['padding' for _ in range(0,ws-1)]\n",
    "                seed_id = id\n",
    "                ws_list.append(id)\n",
    "        else: # 마지막 다이얼로그의 위의 else문에 들어갈 수 없으므로 따로 처리\n",
    "            for i in range(0,len(ws_list)-ws+1):\n",
    "                    ws_df = ws_df.append({'seq' : ws_list[i:i+ws]},ignore_index=True)\n",
    "        return ws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_df = _get_ws_list(df_speaker,8)\n",
    "ws_df.to_csv('./data/KEMDy19/df_speaker_8.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "audio, sr = librosa.load('./audio/Sess01_impro01_F001.wav', sr=16000)\n",
    "audio_len = audio.shape[0]\n",
    "if audio_len < sr*10: # 10초가 안되는 음성이면\n",
    "    audio = librosa.util.fix_length(audio, size=sr*10)\n",
    "else: # 10초가 넘는 음성이면\n",
    "    start = random.randint(audio_len - sr*10)\n",
    "    end = start + sr*10\n",
    "    audio = audio[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchaudio.transforms\n",
    "from constants import F_MAX, F_MIN, HOP_SIZE, N_FFT, N_MELS, SAMPLE_RATE\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "spec = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,\n",
    "                                                        n_fft=N_FFT,\n",
    "                                                        hop_length=HOP_SIZE,\n",
    "                                                        f_min=F_MIN,\n",
    "                                                        f_max=F_MAX,\n",
    "                                                        n_mels=N_MELS,\n",
    "                                                        normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_emb_dic = np.load('./data/KEMDy19/embedding_768.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,4]\n",
    "m = [1,2,3,4,5]\n",
    "\n",
    "# [item for item in x if item not in y]\n",
    "[session for session in m if session not in a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "\n",
    "test_split = str([1,2,3,4])\n",
    "l = ast.literal_eval(test_split)\n",
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4100, 0.1979, 0.7392, 0.2015, 0.9846, 0.2317, 0.6167])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 2, 6]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "y = torch.FloatTensor([1.0,0,0,1,0,1,0])\n",
    "pred = torch.rand((7))\n",
    "print(pred)\n",
    "torch.flip(torch.argsort(pred,dim=-1),dims=(-1,))[0:3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.Tensor([0,3,5])\n",
    "pred = torch.Tensor([2,5,3])\n",
    "\n",
    "\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones([7,7])\n",
    "a = torch.cat((a,torch.zeros(1,7)),dim=0)\n",
    "a[:,0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25f94c46818849e4c2653dbb759cb0511c781ae090a730e8356ade1c3b26f049"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
